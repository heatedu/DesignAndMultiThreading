# Cache System - Interview Discussion Guide

A structured 1-hour interview guide for discussing this cache implementation with an SDE2.

---

## â±ï¸ Time Allocation (60 minutes)

| Phase | Duration | Focus |
|-------|----------|-------|
| Introduction & Problem Statement | 5 min | Understanding requirements |
| Architecture Overview | 10 min | High-level design discussion |
| Deep Dive: Thread Safety | 12 min | Concurrency patterns |
| Deep Dive: LRU Algorithm | 10 min | Data structures & complexity |
| Deep Dive: Write Policies | 8 min | Trade-offs & strategies |
| Extensions & Trade-offs | 10 min | Production concerns |
| Q&A | 5 min | Open discussion |

---

## ğŸ“‹ Phase 1: Introduction & Problem Statement (5 min)

### Opening Question
**Interviewer**: "Walk me through your understanding of this cache system."

### Key Points to Cover
```cpp
// Core Requirements:
âœ“ Key-value cache with limited capacity
âœ“ LRU eviction when full
âœ“ Write-through policy to persistent storage
âœ“ Thread-safe operations
âœ“ "Read your own writes" consistency
âœ“ Extensible design
```

### Candidate Should Mention:
1. **Cache capacity**: Fixed size, needs eviction
2. **Persistence**: Cache + DB storage layers
3. **Concurrency**: Multiple threads accessing simultaneously
4. **Consistency**: Strong consistency for same-key operations

---

## ğŸ—ï¸ Phase 2: Architecture Overview (10 min)

### Diagram Discussion

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Client Code                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cache<K,V>                       â”‚
â”‚  - accessData(key) â†’ Future<V>          â”‚
â”‚  - updateData(key, value) â†’ Future<void>â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚        â”‚        â”‚          â”‚
   â–¼        â–¼        â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Cacheâ”‚ â”‚  DB  â”‚ â”‚Write â”‚ â”‚  Eviction  â”‚
â”‚Stor â”‚ â”‚ Stor â”‚ â”‚Policyâ”‚ â”‚  Algorithm â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚                          â”‚
   â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ConcurrentMapâ”‚    â”‚DoublyLinkedList  â”‚
â”‚<K,V>        â”‚    â”‚  + HashMap<K,Node>â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Questions to Ask Candidate

**Q1**: "Why separate CacheStorage and DBStorage interfaces?"

**Expected Answer**:
- **Separation of concerns**: Cache vs persistent storage
- **Testing**: Mock implementations
- **Flexibility**: Swap storage backends (Redis, Memcached, etc.)
- **Single Responsibility Principle**

**Q2**: "Explain the Strategy Pattern usage here."

**Expected Answer**:
```cpp
// Strategy Pattern allows runtime policy selection:

// Different Write Policies
WritePolicy<K,V>
â”œâ”€â”€ WriteThroughPolicy    // Immediate DB write
â”œâ”€â”€ WriteBackPolicy       // Delayed batch writes
â””â”€â”€ WriteAroundPolicy     // Skip cache, write to DB

// Different Eviction Algorithms
EvictionAlgorithm<K>
â”œâ”€â”€ LRUEvictionAlgorithm  // Least Recently Used
â”œâ”€â”€ LFUEvictionAlgorithm  // Least Frequently Used
â””â”€â”€ FIFOEvictionAlgorithm // First In First Out
```

### Code Walkthrough
```cpp
// Dependency Injection enables Strategy Pattern
Cache<string, string> cache(
    cacheStorage,      // What storage?
    dbStorage,         // What database?
    writePolicy,       // How to write?
    evictionAlg,       // How to evict?
    numExecutors       // How many threads?
);
```

---

## ğŸ”’ Phase 3: Deep Dive - Thread Safety (12 min)

### The Core Challenge
**Q**: "How do you ensure 'read your own writes' consistency?"

### Key Design: KeyBasedExecutor

```cpp
class KeyBasedExecutor {
    ExecutorService[] executors;  // Fixed pool of threads
    
    template<typename K>
    int getExecutorIndexForKey(const K& key) {
        return hash(key) % numExecutors;
    }
    
    template<typename K, typename Func>
    auto submitTask(const K& key, Func&& func) {
        int index = getExecutorIndexForKey(key);
        return executors[index].submit(func);
    }
};
```

### Discussion Points

**1. Thread Affinity Pattern**
```cpp
// Same key always goes to same thread
hash("A") % 4 = 2  â†’ Thread 2
hash("B") % 4 = 1  â†’ Thread 1
hash("A") % 4 = 2  â†’ Thread 2 (again!)

// Guarantees:
âœ“ Operations on "A" are serialized
âœ“ No race conditions for same key
âœ“ "Read your own writes" automatically satisfied
```

**2. Why Not Global Lock?**
```cpp
// âŒ Bad: Global lock
mutex globalLock;
void put(K key, V value) {
    lock_guard<mutex> lock(globalLock);  // Serializes EVERYTHING
    cache[key] = value;
}

// âœ… Good: Key-based locks
void put(K key, V value) {
    int threadId = hash(key) % numThreads;
    executors[threadId].submit([=]() {
        cache[key] = value;  // Only this key blocked
    });
}
```

**3. Cross-Thread Eviction Challenge**

```cpp
// Problem: Key "A" on Thread 2, Key "B" on Thread 1
// Thread 2 needs to evict "B" from cache

// Solution in updateData():
if (currentIndex == evictedIndex) {
    // Same thread, remove directly
    cacheStorage->remove(evictedKey);
} else {
    // Different thread, submit and wait
    auto removalFuture = keyBasedExecutor.submitTask(
        evictedKey, [...]() { 
            cacheStorage->remove(evictedKey); 
        }
    );
    removalFuture.get();  // Wait for completion
}
```

**Q**: "What's the trade-off of key-based affinity?"

**Expected Answer**:
- âœ… **Pros**: No global lock, parallel ops on different keys, ordering guaranteed
- âŒ **Cons**: Hot keys bottleneck one thread, uneven load distribution, limited parallelism

---

## ğŸ“Š Phase 4: Deep Dive - LRU Algorithm (10 min)

### Data Structure Design

```cpp
class LRUEvictionAlgorithm<K> {
    DoublyLinkedList<K> dll;              // Head = LRU, Tail = MRU
    unordered_map<K, DLLNode<K>*> nodeMap;  // O(1) lookup
    mutex mutex;                           // Thread safety
};
```

### Visual Representation

```
Initial State:
dll: [A] â‡„ [B] â‡„ [C]
     LRU           MRU
     
Access key "A":
dll: [B] â‡„ [C] â‡„ [A]
     LRU           MRU
     
Evict:
dll: [C] â‡„ [A]
     LRU    MRU
(B removed)
```

### Implementation Details

**Q**: "Walk me through `keyAccessed()` implementation."

```cpp
void keyAccessed(const K& key) {
    lock_guard<mutex> lock(mutex);
    
    if (nodeMap.contains(key)) {
        // Existing key: move to tail (MRU)
        DLLNode<K>* node = nodeMap[key];
        dll.detachNode(node);      // Remove from current position
        dll.addNodeAtTail(node);   // Add to tail (MRU)
    } else {
        // New key: add to tail
        DLLNode<K>* newNode = new DLLNode<K>(key);
        dll.addNodeAtTail(newNode);
        nodeMap[key] = newNode;
    }
}
```

**Q**: "Why doubly linked list instead of array or single linked list?"

**Expected Answer**:
| Operation | Array | Single LL | Doubly LL |
|-----------|-------|-----------|-----------|
| Access tail | O(1) | O(n) | O(1) |
| Remove head | O(n) | O(1) | O(1) |
| Remove middle | O(n) | O(n)* | O(1) |
| Move to tail | O(n) | O(n) | O(1) |

*Need to find previous node

**Complexity Analysis**:
- `keyAccessed()`: **O(1)** (hash lookup + DLL ops)
- `evictKey()`: **O(1)** (remove head)
- Space: **O(n)** where n = cache capacity

### Edge Cases
```cpp
// Empty cache
evictKey() â†’ nullopt

// Single element
[A] â†’ evict â†’ []

// Capacity = 1
[A] â†’ access(B) â†’ evict A â†’ [B]
```

---

## âœï¸ Phase 5: Deep Dive - Write Policies (8 min)

### Write-Through Implementation

```cpp
class WriteThroughPolicy : public WritePolicy<K,V> {
    void write(const K& key, const V& value,
               CacheStorage<K,V>* cache,
               DBStorage<K,V>* db) override {
        
        // Parallel writes using std::async
        auto cacheFuture = async(launch::async, [&]() {
            cache->put(key, value);
        });
        
        auto dbFuture = async(launch::async, [&]() {
            db->write(key, value);
        });
        
        // Wait for both
        cacheFuture.get();
        dbFuture.get();
    }
};
```

### Comparison of Write Policies

**1. Write-Through** (Implemented)
```
Client â†’ [Cache + DB] (parallel) â†’ Response
         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         Wait for both

âœ“ Strong consistency
âœ“ No data loss
âœ— Higher latency
âœ— DB becomes bottleneck
```

**2. Write-Back** (Future extension)
```
Client â†’ [Cache] â†’ Response (fast!)
         â””â†’ [Queue] â†’ DB (async, batched)

âœ“ Low latency
âœ“ Batch optimization
âœ— Risk of data loss
âœ— Complex failure handling
```

**3. Write-Around** (Future extension)
```
Client â†’ [DB only] â†’ Response
Cache miss â†’ Load from DB â†’ Cache

âœ“ No cache pollution
âœ— Cache miss on next read
âœ— No benefit for write-heavy keys
```

**Q**: "When would you choose Write-Back over Write-Through?"

**Expected Answer**:
- **Write-Back**: High write volume, tolerable data loss risk (logs, metrics)
- **Write-Through**: Financial data, strong consistency required
- **Write-Around**: Write-once-read-never data, large objects

---

## ğŸš€ Phase 6: Extensions & Production Concerns (10 min)

### Extension Ideas

**1. Add LFU (Least Frequently Used)**

```cpp
class LFUEvictionAlgorithm : public EvictionAlgorithm<K> {
    unordered_map<K, int> frequency;
    map<int, set<K>> freqBuckets;  // freq â†’ keys with that freq
    
    void keyAccessed(const K& key) override {
        int oldFreq = frequency[key];
        frequency[key]++;
        
        freqBuckets[oldFreq].erase(key);
        freqBuckets[oldFreq + 1].insert(key);
    }
    
    optional<K> evictKey() override {
        auto [minFreq, keys] = *freqBuckets.begin();
        K evictKey = *keys.begin();
        // Remove from buckets and frequency map
        return evictKey;
    }
};
```

**2. TTL (Time-To-Live) Support**

```cpp
struct CacheEntry<V> {
    V value;
    chrono::time_point<chrono::system_clock> expiry;
};

// In accessData():
if (isExpired(entry)) {
    remove(key);
    throw runtime_error("Key expired");
}
```

**3. Distributed Cache (Advanced)**

```cpp
// Consistent hashing for multi-node cache
class ConsistentHash {
    map<size_t, string> ring;  // hash â†’ node
    
    string getNode(const K& key) {
        size_t hash = hashFunc(key);
        auto it = ring.lower_bound(hash);
        return (it == ring.end()) ? ring.begin()->second : it->second;
    }
};
```

### Production Considerations

**Q**: "What would you change for production?"

**Expected Answers**:

1. **Monitoring & Metrics**
```cpp
class CacheMetrics {
    atomic<uint64_t> hits{0};
    atomic<uint64_t> misses{0};
    atomic<uint64_t> evictions{0};
    
    double hitRatio() { return hits / (hits + misses); }
};
```

2. **Error Handling**
```cpp
// Current: throws exceptions
// Production: Error codes, retry logic, circuit breakers

enum class CacheError {
    KEY_NOT_FOUND,
    CAPACITY_EXCEEDED,
    DB_UNAVAILABLE,
    TIMEOUT
};

Result<V, CacheError> accessData(const K& key);
```

3. **Resource Limits**
```cpp
// Memory limits
size_t maxMemoryBytes;
size_t currentMemoryUsage;

// Connection pooling
DBConnectionPool dbPool(maxConnections);
```

4. **Graceful Shutdown**
```cpp
// Flush dirty entries (for write-back)
// Wait for in-flight operations
// Save cache state to disk
```

---

## â“ Phase 7: Q&A (5 min)

### Common Follow-up Questions

**Q1**: "How would you handle cache stampede?"

**Answer**: Request coalescing - if key is being fetched, queue additional requests
```cpp
unordered_map<K, shared_future<V>> inflightRequests;
```

**Q2**: "How to prevent hot key bottleneck?"

**Answer**: 
- Replicate hot keys across multiple threads
- Use probabilistic data structures (Count-Min Sketch)
- Client-side caching

**Q3**: "How to test this system?"

**Answer**:
- Unit tests: Mock storage implementations
- Concurrency tests: Multiple threads, race conditions
- Load tests: Measure throughput and latency
- Chaos tests: Inject failures (DB down, thread hangs)

---

## ğŸ“ Summary Checklist

By end of interview, candidate should demonstrate understanding of:

- âœ… Strategy Pattern for extensibility
- âœ… Thread safety via key-based affinity
- âœ… LRU algorithm O(1) complexity
- âœ… Trade-offs between write policies
- âœ… Futures for async operations
- âœ… Template-based generic design
- âœ… Production considerations

---

## ğŸ¯ Success Criteria

| Level | Criteria |
|-------|----------|
| **Strong Hire** | Explains all design decisions, suggests improvements, discusses trade-offs deeply |
| **Hire** | Understands architecture, explains LRU & thread safety, answers most questions |
| **Maybe** | Understands basic concepts but struggles with concurrency or trade-offs |
| **No Hire** | Cannot explain key components or design rationale |

---

**Total Time**: 60 minutes  
**Difficulty**: SDE2 Level  
**Focus Areas**: System Design, Concurrency, Data Structures, Design Patterns

